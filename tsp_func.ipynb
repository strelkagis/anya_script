{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_17668\\1661411409.py:3: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "# import pandana\n",
    "# import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import geoalchemy2\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для чтения того, что получили в запросе\n",
    "def select_pg(sql, geom):\n",
    "    return gpd.read_postgis(sql, engine, geom_col = geom, crs = 3857)\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://kb_geo:kYZQK90yvE8aNi54ELINc0yJ1gu6wo7h@\\\n",
    "db03.cluster.strlk.ru/kb_graph')\n",
    "\n",
    "\n",
    "#коннект к базе данных \n",
    "conn = psycopg2.connect(\n",
    "    host=\"db03.cluster.strlk.ru\",\n",
    "    database=\"kb_graph\",\n",
    "    user=\"kb_geo\",\n",
    "    password=\"kYZQK90yvE8aNi54ELINc0yJ1gu6wo7h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# необходимо, чтбоы в таблице с точками, были id графа\n",
    "\n",
    "def tsp_trip(name, graph):\n",
    "\n",
    "    #коннект к бд\n",
    "    conn = psycopg2.connect(\n",
    "    host=\"db03.cluster.strlk.ru\",\n",
    "    database=\"kb_graph\",\n",
    "    user=\"kb_geo\",\n",
    "    password=\"kYZQK90yvE8aNi54ELINc0yJ1gu6wo7h\")\n",
    "\n",
    "\n",
    "    df = select_pg(f'select * from {name}', 'geom')\n",
    "    df.dropna(subset=['owner_id', 'trip'], inplace=True)\n",
    "    df_nodes = df[['owner_id', 'trip']]\n",
    "    df_nodes.drop_duplicates(subset=['owner_id', 'trip'], inplace=True)\n",
    "    df_nodes = df_nodes.reset_index(names = 'level')\n",
    "    # df.merge(df_nodes, on = [['owner_id','nodes']])\n",
    "    df = df.merge(df_nodes, on=['owner_id', 'trip'])\n",
    "    df.to_postgis(f'{name}_tsp', engine, if_exists = 'replace')\n",
    "\n",
    "\n",
    "    # Создание курсора для выполнения запросов\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            query = f\"drop table if exists {name}_trip\"\n",
    "            cur.execute(query)\n",
    "\n",
    "    # Закрытие курсора и соединения\n",
    "    cur.close()\n",
    "\n",
    "    # Создание курсора для выполнения запросов\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "\n",
    "            query = f\"CREATE TABLE {name}_trip (\\\n",
    "                level int,\\\n",
    "                start_vid bigint, \\\n",
    "                end_vid bigint, \\\n",
    "                node  bigint,\\\n",
    "                edge  bigint,\\\n",
    "                geom geometry);\"\n",
    "\n",
    "            cur.execute(query)\n",
    "\n",
    "    # Закрытие курсора и соединения\n",
    "    cur.close()\n",
    "\n",
    "    # Использование параметров для корректной подстановки значения переменной в запрос\n",
    "    for level in df_nodes['level']:\n",
    "        print(level)\n",
    "        try:\n",
    "            with conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    query = f\"insert into {name}_trip (level, start_vid, end_vid, node, edge, geom)\\\n",
    "                                with path as\\\n",
    "                                    (SELECT {level} as level, array_agg(node) as node FROM\\\n",
    "                                    pgr_TSP($$SELECT * FROM pgr_dijkstraCostMatrix('SELECT id, source, target, cost, reverse_cost FROM {graph}',\\\n",
    "                                    (SELECT array_agg(node_id) FROM {name}_tsp WHERE level = {level}), directed := false) $$, randomize := false)\\\n",
    "                                    group by level),\\\n",
    "                                route as (\\\n",
    "                                    SELECT  start_vid , end_vid , node, edge from pgr_dijkstraVia('SELECT id, source, target, cost, reverse_cost FROM {graph}',\\\n",
    "                                        (SELECT node FROM path WHERE level = {level}), directed := false))\\\n",
    "                                select {level} as level, route.*, geom from route join {graph} on edge = id\"\n",
    "                    # print(index)\n",
    "                                    \n",
    "                    cur.execute(query)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Закрытие курсора и соединения\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция разделения по временным кластерам \n",
    "\n",
    "def cluster(name, time):\n",
    "\n",
    "    #подлкючаемся к базе данных\n",
    "    conn = psycopg2.connect(\n",
    "    host=\"db03.cluster.strlk.ru\",\n",
    "    database=\"kb_graph\",\n",
    "    user=\"kb_geo\",\n",
    "    password=\"kYZQK90yvE8aNi54ELINc0yJ1gu6wo7h\")\n",
    "\n",
    "\n",
    "    #считываем таблицу \n",
    "    df = select_pg(f'select id, geom, in_out, owner_id, date_time, url, city_left, city_right, month, user_region, photo_region from {name}', 'geom')\n",
    "    #переводим время и дату в формате времени и даты\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    #выделяем месяц\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['ts_d'] = 0\n",
    "    #формат - количество секунд с определенной даты (время в секундах с начала эпохи Unix)\n",
    "    df['ts'] = df['date_time'].progress_apply(lambda x: x.timestamp())\n",
    "    #сохраняются те строки 'ts' имеет более 2 значений (берем только те строки, которые имеют две разные отметки времени)\n",
    "    df = df.groupby(['owner_id']).filter(lambda x: x['ts'].count() > 2)\n",
    "\n",
    "    #функция для группировкт объектов\n",
    "    #epsilon - количество секунд, которые могут разделять один кластер \n",
    "    def groupby_cluster(X):\n",
    "        db = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, cluster_selection_epsilon=1*24*60*60*1*time).fit(X) \n",
    "        return pd.DataFrame(db.labels_, index=X.index)\n",
    "\n",
    "    #группировку данных по 'owner_id', а затем применение к сгруппированных значениям функции 'groupby_cluster' \n",
    "    # Результаты кластеризации сохраняются в новом DataFrame 'df_cluster', который содержит метки кластеров для каждой строки в исходном DataFrame 'df'.\n",
    "    df_cluster = df.groupby(['owner_id'], as_index=False)[['ts', 'ts_d']].progress_apply(groupby_cluster)\n",
    "\n",
    "    #приджойниваем номера кластеров к фотографиям \n",
    "    df = df.join(df_cluster[0]).rename(columns={0: \"cluster\"})\n",
    "    #убираем те фотографии, где метка кластера равна -1, так как это выбросы (например, только одна фотография сделана за указанный период)\n",
    "    df = df[df['cluster'] != -1]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединение двух вышестоящих функций в одну\n",
    "# для этой функции должен быть связный граф. сейчас используется baikal_edge_graph_connected_all\n",
    "\n",
    "def tsp(name, time):\n",
    "\n",
    "    #подлкючаемся к базе данных\n",
    "    conn = psycopg2.connect(\n",
    "    host=\"db03.cluster.strlk.ru\",\n",
    "    database=\"kb_graph\",\n",
    "    user=\"kb_geo\",\n",
    "    password=\"kYZQK90yvE8aNi54ELINc0yJ1gu6wo7h\")\n",
    "\n",
    "\n",
    "    #считываем таблицу \n",
    "    df = select_pg(f'select id, geom, id_gis, in_out, owner_id, date_time, url, city, month, user_region, photo_region, node_id, node_geom from {name}', 'geom')\")\n",
    "    #переводим время и дату в формате времени и даты\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    #выделяем месяц\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['ts_d'] = 0\n",
    "    #формат - количество секунд с определенной даты (время в секундах с начала эпохи Unix)\n",
    "    df['ts'] = df['date_time'].progress_apply(lambda x: x.timestamp())\n",
    "    #сохраняются те строки 'ts' имеет более 2 значений (берем только те строки, которые имеют две разные отметки времени)\n",
    "    df = df.groupby(['owner_id']).filter(lambda x: x['ts'].count() > 2)\n",
    "\n",
    "    # db = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, cluster_selection_epsilon=1*24*60*60*1*time).fit(X)\n",
    "    # hdbscan_df = pd.DataFrame(db.labels_, index=X.index)\n",
    "\n",
    "    #функция для группировкт объектов\n",
    "    #epsilon - количество секунд, которые могут разделять один кластер \n",
    "    def groupby_cluster(X):\n",
    "        db = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, cluster_selection_epsilon=1*24*60*60*1*time).fit(X) \n",
    "        return pd.DataFrame(db.labels_, index=X.index)\n",
    "\n",
    "    #группировку данных по 'owner_id', а затем применение к сгруппированных значениям функции 'groupby_cluster' \n",
    "    # Результаты кластеризации сохраняются в новом DataFrame 'df_cluster', который содержит метки кластеров для каждой строки в исходном DataFrame 'df'.\n",
    "    df_cluster = df.groupby(['owner_id'], as_index=False)[['ts', 'ts_d']].progress_apply(groupby_cluster)\n",
    "\n",
    "    #приджойниваем номера кластеров к фотографиям \n",
    "    df = df.join(df_cluster[0]).rename(columns={0: \"cluster\"})\n",
    "    #убираем те фотографии, где метка кластера равна -1, так как это выбросы (например, только одна фотография сделана за указанный период)\n",
    "    df = df[df['cluster'] != -1]\n",
    "\n",
    "    # группируем по id пользоватнля и номеру кластера, считаем количество уникальных node id \n",
    "    count = df.groupby(['owner_id', 'cluster'])['node_id'].count()\n",
    "    count = count.reset_index()\n",
    "    #оставляем только тех 'owner_id', 'cluster' у которыз более двух различных узлов в маршруте \n",
    "    count_more_two = count[count['node_id'] >= 2]\n",
    "\n",
    "    #приджойниваем полученный набор данных к фотографиям и остаются только те 'owner_id', 'cluster' у которых больше двух уникальных node_id\n",
    "    df = df.merge(count_more_two, on=['owner_id', 'cluster'], how='inner', indicator=True)\n",
    "\n",
    "    #создаем датафрэйм с колонами 'owner_id', 'cluster' и списком посещенных node_id_x\n",
    "    df_nodes = df.groupby(['owner_id', 'cluster'])['node_id_x'].apply(list).reset_index()\n",
    "    # df_nodes = df_nodes.reset_index()\n",
    "\n",
    "\n",
    "    df = df.merge(df_nodes[['index', 'owner_id', 'cluster']], on = ['owner_id', 'cluster'], how = 'inner')\n",
    "    df.to_postgis(f'{name}_tsp', engine, if_exists = 'replace')\n",
    "\n",
    "    # Создание курсора для выполнения запросов\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Этот код выполняет SQL-запрос на удаление таблицы с названием \"{name}_route\", \n",
    "    # если она существует в базе данных, подключенной к курсору \"cur\". \n",
    "    # Оператор \"with\" используется для обеспечения правильного закрытия соединения с базой данных после выполнения запроса.\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            query = f\"drop table if exists {name}_route\"\n",
    "            cur.execute(query)\n",
    "\n",
    "    # Закрытие курсора и соединения\n",
    "    cur.close()\n",
    "\n",
    "    # Создание курсора для выполнения запросов\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # query = f\"CREATE TABLE if not exists {name}_path(\\\n",
    "            #     index integer,\\\n",
    "            #     node ARRAY INTEGER)\"\n",
    "            # cur.execute(query)\n",
    "\n",
    "            #создаем таблицу route в которой есть:\n",
    "            # index, начальная и конечная вершины пути между точками, node - номер узла по которому проходим, edge - номер ребра по которому проходим, geom - геометрия ребра\n",
    "            # df_nodes['index'] - уникальные значение index и owner id\n",
    "            query = f\"CREATE TABLE {name}_route(\\\n",
    "                index int,\\\n",
    "                start_vid bigint, \\\n",
    "                end_vid bigint, \\\n",
    "                node  bigint,\\\n",
    "                edge  bigint,\\\n",
    "                geom geometry);\"\n",
    "\n",
    "            cur.execute(query)\n",
    "\n",
    "    # Закрытие курсора и соединения\n",
    "    cur.close()\n",
    "\n",
    "    # Использование параметров для корректной подстановки значения переменной в запрос\n",
    "    # пробегаемся по каждому уникальному значению index \n",
    "    for index in df_nodes['index']:\n",
    "        try:\n",
    "            with conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    #инсертим знаение в таблицу route \n",
    "                    query = f\"insert into {name}_route\\\n",
    "                                with path as\\\n",
    "                                    -- выборка данных из таблицы, содержащей узлы графа, которые нужно обойти в заданном порядке \\\n",
    "                                    (SELECT {index} as index, array_agg(node) as node FROM \\\n",
    "                                    --выполнение алгоритма TSP с помощью функции pgr_TSP\\\n",
    "                                    -- Она использует матрицу стоимости, созданную с помощью функции pgr_dijkstraCostMatrix\\\n",
    "                                    -- для нахождения оптимального маршрута, проходящего через все узлы в заданном порядке.\\\n",
    "                                    pgr_TSP($$SELECT * FROM pgr_dijkstraCostMatrix('SELECT id, source, target, cost, reverse_cost FROM baikal_edge_graph_connected_all', \\\n",
    "                                    --выборка узлов из таблицы, которые нужно обойти в задаче TSP \\\n",
    "                                    (SELECT array_agg(node_id_x) FROM {name}_tsp WHERE index = {index}), directed := false) $$, randomize := false)\\\n",
    "                                    group by index),\\\n",
    "                                    edge_route as (\\\n",
    "                                    -- выполнение алгоритма Dijkstra с помощью функции pgr_dijkstraVia для поиска кратчайшего маршрута между каждой парой узлов.\\\n",
    "                                    SELECT {index} as index, start_vid , end_vid , node, edge from pgr_dijkstraVia('SELECT id, source, target, cost, reverse_cost FROM baikal_edge_graph_connected_all', \\\n",
    "                                        (SELECT node FROM path WHERE index = {index}), directed := false))\\\n",
    "                                    -- объединение  таблиц edge_route и baikal_edge_graph_connected_all для получения итогового маршрута с геометрическими данными.\\\n",
    "                                    select edge.*, graph.geom as geom\\\n",
    "                                    from edge_route as edge join baikal_edge_graph_connected_all as graph on edge = graph.id ;\"\n",
    "                    \n",
    "                    print(index)\n",
    "                                    \n",
    "                    cur.execute(query)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Закрытие курсора и соединения\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    return df_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
